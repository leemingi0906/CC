import argparse
import datetime
import random
import time
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from crowd_datasets import build_dataset
from engine import train_one_epoch, evaluate_crowd_no_overlap
from models import build_model
import os
from tensorboardX import SummaryWriter
import warnings
import numpy as np
import util.misc as utils
import gc

# [ì•ˆì •í™” ì„¤ì •] cuBLAS ì—°ì‚° ì—ëŸ¬ ë°©ì§€
os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
warnings.filterwarnings('ignore')

# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì • í•¨ìˆ˜
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# DataLoader ì›Œì»¤ ì‹œë“œ ê³ ì •
def seed_worker(worker_id):
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)

def get_args_parser():
    parser = argparse.ArgumentParser('P2PNet Training for RTX 3090 Stability', add_help=False)
    
    parser.add_argument('--lr', default=1e-4, type=float)
    parser.add_argument('--lr_backbone', default=1e-5, type=float)
    parser.add_argument('--batch_size', default=2, type=int) 
    parser.add_argument('--weight_decay', default=1e-4, type=float)
    parser.add_argument('--epochs', default=3500, type=int)
    parser.add_argument('--lr_drop', default=3500, type=int)
    parser.add_argument('--clip_max_norm', default=0.1, type=float)

    # NPoint ì„¤ì •
    # parser.add_argument('--use_npoint', action='store_true', default=True)
    parser.add_argument('--alpha', default=0.0, type=float)

    # ëª¨ë¸ ì•„í‚¤í…ì²˜
    parser.add_argument('--backbone', default='vgg16_bn', type=str)
    parser.add_argument('--row', default=2, type=int)
    parser.add_argument('--line', default=2, type=int)
    parser.add_argument('--set_cost_class', default=1, type=float)
    parser.add_argument('--set_cost_point', default=0.05, type=float)
    parser.add_argument('--point_loss_coef', default=0.0002, type=float)
    parser.add_argument('--eos_coef', default=0.5, type=float)

    # ê²½ë¡œ ì„¤ì •
    parser.add_argument('--data_root', default='/home/kimsooyeon/Downloads/SHT', help='ë°ì´í„°ì…‹ ê²½ë¡œ')
    parser.add_argument('--dataset_file', default='SHHA')
    parser.add_argument('--output_dir', default='', help='ìë™ ìƒì„±')
    parser.add_argument('--checkpoints_dir', default='', help='ìë™ ìƒì„±')
    parser.add_argument('--tensorboard_dir', default='', help='ìë™ ìƒì„±')

    parser.add_argument('--seed', default=42, type=int)
    parser.add_argument('--resume', default='', help='ê°€ì¤‘ì¹˜ ì¬ì‹œì‘ ê²½ë¡œ')
    parser.add_argument('--num_workers', default=2, type=int)
    # [ìˆ˜ì •] í‰ê°€ ì£¼ê¸°ë¥¼ ê¸°ë³¸ 5ë¡œ ì„¤ì •í•˜ì—¬ í›ˆë ¨ íš¨ìœ¨ì„±ì„ ë†’ì„
    parser.add_argument('--eval_freq', default=5, type=int)

    return parser

def main(args):
    set_seed(args.seed)

    device = torch.device('cuda')
    model, criterion = build_model(args, training=True)
    model.to(device)
    criterion.to(device)

    if torch.cuda.device_count() > 1:
        print(f"âœ… Using DataParallel with {torch.cuda.device_count()} GPUs")
        model = torch.nn.DataParallel(model)  # model.module ë¡œ ì ‘ê·¼ í•„ìš”í•´ì§ˆ ìˆ˜ ìˆìŒ
        model_without_ddp = model.module
    else:
        model_without_ddp = model


    #if args.gpu_id:
    #    os.environ["CUDA_VISIBLE_DEVICES"] = '{}'.format(args.gpu_id)

    if not os.path.exists(args.data_root):
        print(f"âŒ ì˜¤ë¥˜: ë°ì´í„° ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.data_root}")
        return

    # gc.collect()
    # torch.cuda.empty_cache()

    suffix = f"npoint_a{str(args.alpha).replace('.', '_')}_seed{args.seed}" if args.alpha != 0 else f"baseline_seed{args.seed}"
    os.makedirs(f"./my_exp/exp-{suffix}", exist_ok=True)
    if not args.output_dir: args.output_dir = f'./my_exp/exp-{suffix}/logs_{suffix}'
    if not args.checkpoints_dir: args.checkpoints_dir = f'./my_exp/exp-{suffix}/ckpt_{suffix}'
    if not args.tensorboard_dir: args.tensorboard_dir = f'./my_exp/exp-{suffix}/runs_{suffix}'

    for d in [args.output_dir, args.checkpoints_dir]:
        if not os.path.exists(d): os.makedirs(d)

    optimizer = torch.optim.Adam([
        {"params": [p for n, p in model_without_ddp.named_parameters() if "backbone" not in n and p.requires_grad]},
        {"params": [p for n, p in model_without_ddp.named_parameters() if "backbone" in n and p.requires_grad], "lr": args.lr_backbone},
    ], lr=args.lr, weight_decay=args.weight_decay)
    
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)

    loading_data = build_dataset(args=args)
    train_set, val_set = loading_data(args.data_root)
    
    # if hasattr(train_set, 'use_npoint'):
        # train_set.use_npoint = args.use_npoint
    train_set.alpha = args.alpha

    # g = torch.Generator()
    # g.manual_seed(args.seed)

    data_loader_train = DataLoader(
        train_set, batch_size=args.batch_size, shuffle=True,
        collate_fn=utils.collate_fn_crowd, num_workers=args.num_workers, 
        pin_memory=True, worker_init_fn=seed_worker
    )
    
    data_loader_val = DataLoader(
        val_set, 1, shuffle=False,
        collate_fn=utils.collate_fn_crowd, num_workers=args.num_workers, 
        pin_memory=True
    )

    writer = SummaryWriter(args.tensorboard_dir)
    run_log_name = os.path.join(args.output_dir, 'run_log.txt')
    mae_list = []
    
    print(f"âœ¨ í•™ìŠµ ì‹œì‘ (MAE/MSE í‰ê°€ ì£¼ê¸°: {args.eval_freq} ì—í­)")
    
    for epoch in range(args.epochs):
        try:
            t1 = time.time()
            stat = train_one_epoch(model, criterion, data_loader_train, optimizer, device, epoch, args.clip_max_norm)
            t2 = time.time()

            # í•™ìŠµ ë¡œê·¸ (Train Loss)
            log_text = f'[Ep {epoch}] LR: {optimizer.param_groups[0]["lr"]:.7f} | Loss: {stat["loss"]:.4f} | Time: {t2-t1:.1f}s'
            print(log_text)
            with open(run_log_name, "a") as f: f.write(log_text + "\n")
            
            writer.add_scalar('loss/total', stat['loss'], epoch)
            lr_scheduler.step()

            # í‰ê°€ ë° MAE/MSE ì¶œë ¥ (5ì—í­ ì£¼ê¸°)
            if epoch % args.eval_freq == 0 and epoch > 0:
                torch.cuda.synchronize()
                print(f"ğŸ” ì—í­ {epoch} í‰ê°€ ìˆ˜í–‰ ì¤‘...")
                result = evaluate_crowd_no_overlap(model_without_ddp, data_loader_val, device)
                
                mae, mse = result[0], result[1]
                mae_list.append(mae)
                best_mae = np.min(mae_list)
                
                # ìˆ˜ì¹˜ ì¶œë ¥ (í„°ë¯¸ë„ ë° íŒŒì¼)
                eval_log = f"--- [Evaluation] Epoch {epoch} | MAE: {mae:.2f} | MSE: {mse:.2f} | Best MAE: {best_mae:.2f}"
                print(eval_log)
                with open(run_log_name, "a") as f:
                    f.write(eval_log + "\n")
                
                writer.add_scalar('metric/mae', mae, epoch)
                writer.add_scalar('metric/mse', mse, epoch)

                # ìµœê³  ì„±ëŠ¥ ê°±ì‹  ì‹œ ëª¨ë¸ ì €ì¥
                if mae <= best_mae:
                    torch.save({'model': model_without_ddp.state_dict(), 'epoch': epoch, 'mae': mae}, 
                               os.path.join(args.checkpoints_dir, 'best_mae.pth'))
                    print(f"ğŸ”¥ ì‹ ê¸°ë¡ ë‹¬ì„±! ëª¨ë¸ ì €ì¥ ì™„ë£Œ.")

        except RuntimeError as e:
            if 'out of memory' in str(e):
                print(f"âš ï¸ ëŸ°íƒ€ì„ ì—ëŸ¬ ë°œìƒ: {e}. ìºì‹œ ì •ë¦¬ í›„ ë‹¤ìŒ ì—í­ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                gc.collect()
                torch.cuda.empty_cache()
                continue
            else: 
                raise e

    writer.close()

if __name__ == '__main__':
    args = get_args_parser().parse_args()
    main(args)