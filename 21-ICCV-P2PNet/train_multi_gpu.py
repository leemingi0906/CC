import argparse
import datetime
import random
import time
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from crowd_datasets import build_dataset
from engine import train_one_epoch, evaluate_crowd_no_overlap
from models import build_model
import os
from tensorboardX import SummaryWriter
import warnings
import numpy as np
import util.misc as utils

warnings.filterwarnings('ignore')

def get_args_parser():
    parser = argparse.ArgumentParser('P2PNet Multi-GPU Training with NPoint', add_help=False)
    parser.add_argument('--lr', default=1e-4, type=float)
    parser.add_argument('--lr_backbone', default=1e-5, type=float)
    # 4ê°œ GPU ì‚¬ìš© ì‹œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ 16 ê¶Œìž¥
    parser.add_argument('--batch_size', default=16, type=int) 
    parser.add_argument('--weight_decay', default=1e-4, type=float)
    parser.add_argument('--epochs', default=3500, type=int)
    parser.add_argument('--lr_drop', default=3500, type=int)
    parser.add_argument('--clip_max_norm', default=0.1, type=float,
                        help='gradient clipping max norm')

    # [NPoint ì„¤ì •]
    parser.add_argument('--use_npoint', action='store_true', help='NPoint ì¦ê°• í™œì„±í™” ì—¬ë¶€')
    parser.add_argument('--alpha', default=0.5, type=float, help='NPoint ë…¸ì´ì¦ˆ ê°•ë„ (alpha)')

    # ëª¨ë¸ íŒŒë¼ë¯¸í„°
    parser.add_argument('--frozen_weights', type=str, default=None)
    parser.add_argument('--backbone', default='vgg16_bn', type=str)
    parser.add_argument('--set_cost_class', default=1, type=float)
    parser.add_argument('--set_cost_point', default=0.05, type=float)
    parser.add_argument('--point_loss_coef', default=0.0002, type=float)
    parser.add_argument('--eos_coef', default=0.5, type=float)
    parser.add_argument('--row', default=2, type=int)
    parser.add_argument('--line', default=2, type=int)

    # ë°ì´í„°ì…‹ ì„¤ì •
    parser.add_argument('--dataset_file', default='SHHA')
    parser.add_argument('--data_root', default='/home/mingi/Downloads/SHT', help='ë°ì´í„°ì…‹ ê²½ë¡œ')
    parser.add_argument('--output_dir', default='./logs_npoint_a05', help='ë¡œê·¸ ì €ìž¥ ê²½ë¡œ')
    parser.add_argument('--checkpoints_dir', default='./ckpt_npoint_a05', help='ì²´í¬í¬ì¸íŠ¸ ì €ìž¥ ê²½ë¡œ')
    parser.add_argument('--tensorboard_dir', default='./runs_npoint_a05')

    parser.add_argument('--seed', default=42, type=int)
    parser.add_argument('--resume', default='', help='ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìž¬ì‹œìž‘')
    parser.add_argument('--start_epoch', default=0, type=int, metavar='N')
    parser.add_argument('--eval', action='store_true')
    # [ìˆ˜ì •] ë©€í‹°í”„ë¡œì„¸ì‹± ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ê¸°ë³¸ ì›Œì»¤ ìˆ˜ë¥¼ 4ë¡œ í•˜í–¥ ì¡°ì •
    parser.add_argument('--num_workers', default=4, type=int)
    parser.add_argument('--eval_freq', default=5, type=int)
    parser.add_argument('--gpu_id', default=0, type=int)

    return parser

def main(args):
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  GPU ì„¤ì •
    os.environ["CUDA_VISIBLE_DEVICES"] = ",".join(map(str, range(torch.cuda.device_count())))
    device = torch.device('cuda')

    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
    
    run_log_name = os.path.join(args.output_dir, 'run_log.txt')
    with open(run_log_name, "w") as f:
        f.write(f"ì‹œìž‘ ì‹œê°„: {time.strftime('%c')}\nì„¤ì •: {args}\n")

    seed = args.seed
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)

    # ëª¨ë¸ ë¹Œë“œ
    model, criterion = build_model(args, training=True)
    model.to(device)
    
    if torch.cuda.device_count() > 1:
        print(f"ðŸš€ {torch.cuda.device_count()}ê°œì˜ GPUë¥¼ ì‚¬ìš©í•˜ì—¬ DataParallel í•™ìŠµì„ ì‹œìž‘í•©ë‹ˆë‹¤!")
        model = nn.DataParallel(model)
        model_without_ddp = model.module
    else:
        model_without_ddp = model

    criterion.to(device)

    param_dicts = [
        {"params": [p for n, p in model_without_ddp.named_parameters() if "backbone" not in n and p.requires_grad]},
        {"params": [p for n, p in model_without_ddp.named_parameters() if "backbone" in n and p.requires_grad], "lr": args.lr_backbone},
    ]
    optimizer = torch.optim.Adam(param_dicts, lr=args.lr, weight_decay=args.weight_decay)
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)

    # Resume ë¡œì§
    if args.resume:
        if os.path.exists(args.resume):
            print(f"ðŸ“‚ ê°€ì¤‘ì¹˜ íŒŒì¼ ë¡œë“œ ì¤‘: {args.resume}")
            checkpoint = torch.load(args.resume, map_location='cpu')
            state_dict = checkpoint['model'] if 'model' in checkpoint else checkpoint
            new_state_dict = {k[7:] if k.startswith('module.') else k: v for k, v in state_dict.items()}
            model_without_ddp.load_state_dict(new_state_dict)
            if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:
                optimizer.load_state_dict(checkpoint['optimizer'])
                lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])
                args.start_epoch = checkpoint['epoch'] + 1
        else:
            print(f"âš ï¸ ê²½ê³ : '{args.resume}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ì‹œìž‘í•©ë‹ˆë‹¤.")

    # ë°ì´í„° ë¡œë”©
    loading_data = build_dataset(args=args)
    train_set, val_set = loading_data(args.data_root)
    
    if hasattr(train_set, 'use_npoint'):
        train_set.use_npoint = args.use_npoint
        if hasattr(train_set, 'alpha'):
            train_set.alpha = args.alpha
        status = "í™œì„±í™”" if args.use_npoint else "ë¹„í™œì„±í™”"
        print(f"âš ï¸ NPoint ì¦ê°•ì´ {status}ë˜ì—ˆìŠµë‹ˆë‹¤. (alpha={args.alpha})")

    # [ìˆ˜ì •] pin_memory=Falseë¡œ ì„¤ì •í•˜ì—¬ ê³µìœ  ë©”ëª¨ë¦¬ ë¶€í•˜ë¥¼ ì¤„ìž„
    data_loader_train = DataLoader(train_set, batch_size=args.batch_size, shuffle=True,
                                   collate_fn=utils.collate_fn_crowd, num_workers=args.num_workers,
                                   pin_memory=False)
    data_loader_val = DataLoader(val_set, 1, shuffle=False,
                                    collate_fn=utils.collate_fn_crowd, num_workers=args.num_workers,
                                    pin_memory=False)

    writer = SummaryWriter(args.tensorboard_dir)
    print(f"í•™ìŠµì„ ì‹œìž‘í•©ë‹ˆë‹¤ (ì•ŒíŒŒ={args.alpha})...")
    mae_list = []
    
    for epoch in range(args.start_epoch, args.epochs):
        t1 = time.time()
        stat = train_one_epoch(model, criterion, data_loader_train, optimizer, device, epoch, args.clip_max_norm)
        t2 = time.time()

        epoch_log = f'[ì—í­ {epoch}][í•™ìŠµë¥  {optimizer.param_groups[0]["lr"]:.7f}][ì†Œìš”ì‹œê°„ {t2 - t1:.2f}s]'
        print(epoch_log)
        with open(run_log_name, "a") as f:
            f.write(epoch_log + f" loss: {stat['loss']:.4f}\n")
        
        writer.add_scalar('loss/total', stat['loss'], epoch)
        lr_scheduler.step()

        if not os.path.exists(args.checkpoints_dir):
            os.makedirs(args.checkpoints_dir)
        
        torch.save({
            'model': model_without_ddp.state_dict(),
            'optimizer': optimizer.state_dict(),
            'lr_scheduler': lr_scheduler.state_dict(),
            'epoch': epoch,
        }, os.path.join(args.checkpoints_dir, 'latest.pth'))
        
        if epoch % args.eval_freq == 0 and epoch > 0:
            result = evaluate_crowd_no_overlap(model_without_ddp, data_loader_val, device)
            mae_list.append(result[0])
            eval_log = f"MAE: {result[0]:.2f}, MSE: {result[1]:.2f}, Best MAE: {np.min(mae_list):.2f}"
            print(f"--- í…ŒìŠ¤íŠ¸ ê²°ê³¼: {eval_log}")
            with open(run_log_name, "a") as f:
                f.write(f"TEST: {eval_log}\n")
            writer.add_scalar('metric/mae', result[0], epoch)

            if result[0] <= np.min(mae_list):
                torch.save({'model': model_without_ddp.state_dict(), 'epoch': epoch}, 
                           os.path.join(args.checkpoints_dir, 'best_mae.pth'))

    writer.close()

if __name__ == '__main__':
    parser = argparse.ArgumentParser('P2PNet Training', parents=[get_args_parser()])
    args = parser.parse_args()
    main(args)
