import argparse
import datetime
import random
import time
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from crowd_datasets import build_dataset
from engine import train_one_epoch, evaluate_crowd_no_overlap
from models import build_model
import os
from tensorboardX import SummaryWriter
import warnings
import numpy as np
import util.misc as utils
import gc

# [ì•ˆì •í™” ì„¤ì •] cuBLAS ì—°ì‚° ì—ëŸ¬ ë°©ì§€
os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
warnings.filterwarnings('ignore')

# ìž¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì • í•¨ìˆ˜
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# DataLoader ì›Œì»¤ ì‹œë“œ ê³ ì •
def seed_worker(worker_id):
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)

def get_args_parser():
    parser = argparse.ArgumentParser('P2PNet Training for RTX 3090 Stability', add_help=False)
    
    parser.add_argument('--lr', default=1e-4, type=float)
    parser.add_argument('--lr_backbone', default=1e-5, type=float)
    parser.add_argument('--batch_size', default=2, type=int) 
    parser.add_argument('--weight_decay', default=1e-4, type=float)
    parser.add_argument('--epochs', default=3500, type=int)
    parser.add_argument('--lr_drop', default=3500, type=int)
    parser.add_argument('--clip_max_norm', default=0.1, type=float)

    # NPoint ì„¤ì •
    parser.add_argument('--use_npoint', action='store_true', help='NPoint ì¦ê°• í™œì„±í™” (alphaê°€ 0ë³´ë‹¤ í¬ë©´ ìžë™ í™œì„±í™”)')
    parser.add_argument('--alpha', default=0.0, type=float, help='NPoint ë…¸ì´ì¦ˆ ê°•ë„')

    # ëª¨ë¸ ì•„í‚¤í…ì²˜
    parser.add_argument('--backbone', default='vgg16_bn', type=str)
    parser.add_argument('--row', default=2, type=int)
    parser.add_argument('--line', default=2, type=int)
    parser.add_argument('--set_cost_class', default=1, type=float)
    parser.add_argument('--set_cost_point', default=0.05, type=float)
    parser.add_argument('--point_loss_coef', default=0.0002, type=float)
    parser.add_argument('--eos_coef', default=0.5, type=float)

    # ê²½ë¡œ ì„¤ì •
    parser.add_argument('--data_root', default='/home/kimsooyeon/Downloads/SHT', help='ë°ì´í„°ì…‹ ê²½ë¡œ')
    parser.add_argument('--dataset_file', default='SHHA', help='ë°ì´í„°ì…‹ ì´ë¦„ (SHHA ë˜ëŠ” SHHB)')
    parser.add_argument('--output_dir', default='', help='ìžë™ ìƒì„±')
    parser.add_argument('--checkpoints_dir', default='', help='ìžë™ ìƒì„±')
    parser.add_argument('--tensorboard_dir', default='', help='ìžë™ ìƒì„±')

    parser.add_argument('--seed', default=42, type=int)
    parser.add_argument('--resume', default='', help='ê°€ì¤‘ì¹˜ ìž¬ì‹œìž‘ ê²½ë¡œ')
    parser.add_argument('--num_workers', default=2, type=int)
    parser.add_argument('--eval_freq', default=5, type=int)

    return parser

def main(args):
    set_seed(args.seed)

    device = torch.device('cuda')
    model, criterion = build_model(args, training=True)
    model.to(device)
    criterion.to(device)

    if torch.cuda.device_count() > 1:
        print(f"âœ… Using DataParallel with {torch.cuda.device_count()} GPUs")
        model = torch.nn.DataParallel(model)
        model_without_ddp = model.module
    else:
        model_without_ddp = model

    if not os.path.exists(args.data_root):
        print(f"âŒ ì˜¤ë¥˜: ë°ì´í„° ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.data_root}")
        return

    # [í•µì‹¬ ìˆ˜ì •] suffixì— args.dataset_fileì„ ì¶”ê°€í•˜ì—¬ í´ë” í˜¼ì„ ì„ ë°©ì§€í•©ë‹ˆë‹¤.
    aug_suffix = f"a{str(args.alpha).replace('.', '_')}" if args.alpha > 0 else "baseline"
    suffix = f"{args.dataset_file}_{aug_suffix}_seed{args.seed}"
    
    exp_path = f"./my_exp/exp-{suffix}"
    if not args.output_dir: args.output_dir = os.path.join(exp_path, f'logs_{suffix}')
    if not args.checkpoints_dir: args.checkpoints_dir = os.path.join(exp_path, f'ckpt_{suffix}')
    if not args.tensorboard_dir: args.tensorboard_dir = os.path.join(exp_path, f'runs_{suffix}')

    for d in [args.output_dir, args.checkpoints_dir]:
        if not os.path.exists(d): os.makedirs(d, exist_ok=True)

    optimizer = torch.optim.Adam([
        {"params": [p for n, p in model_without_ddp.named_parameters() if "backbone" not in n and p.requires_grad]},
        {"params": [p for n, p in model_without_ddp.named_parameters() if "backbone" in n and p.requires_grad], "lr": args.lr_backbone},
    ], lr=args.lr, weight_decay=args.weight_decay)
    
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)

    # ë°ì´í„° ë¡œë“œ (args ì „ë‹¬)
    loading_data = build_dataset(args=args)
    train_set, val_set = loading_data(args.data_root, args)
    
    # NPoint ìµœì¢… ìƒíƒœ ì£¼ìž…
    train_set.alpha = args.alpha
    if args.alpha > 0:
        train_set.use_npoint = True
        npoint_status = f"í™œì„±í™” (Alpha: {args.alpha})"
    else:
        train_set.use_npoint = False
        npoint_status = "ë¹„í™œì„±í™” (Baseline)"

    data_loader_train = DataLoader(
        train_set, batch_size=args.batch_size, shuffle=True,
        collate_fn=utils.collate_fn_crowd, num_workers=args.num_workers, 
        pin_memory=True, worker_init_fn=seed_worker
    )
    
    data_loader_val = DataLoader(
        val_set, 1, shuffle=False,
        collate_fn=utils.collate_fn_crowd, num_workers=args.num_workers, 
        pin_memory=True
    )

    writer = SummaryWriter(args.tensorboard_dir)
    run_log_name = os.path.join(args.output_dir, 'run_log.txt')
    mae_list = []
    
    print(f"âœ¨ í•™ìŠµ ì‹œìž‘ [ë°ì´í„°ì…‹: {args.dataset_file} | NPoint: {npoint_status} | Seed: {args.seed}]")
    
    for epoch in range(args.epochs):
        try:
            gc.collect()
            torch.cuda.empty_cache()

            t1 = time.time()
            stat = train_one_epoch(model, criterion, data_loader_train, optimizer, device, epoch, args.clip_max_norm)
            t2 = time.time()

            log_text = f'[Ep {epoch}] LR: {optimizer.param_groups[0]["lr"]:.7f} | Loss: {stat["loss"]:.4f} | {t2-t1:.1f}s'
            print(log_text)
            with open(run_log_name, "a") as f: f.write(log_text + "\n")
            
            writer.add_scalar('loss/total', stat['loss'], epoch)
            lr_scheduler.step()

            if epoch % args.eval_freq == 0 and epoch > 0:
                torch.cuda.synchronize()
                result = evaluate_crowd_no_overlap(model_without_ddp, data_loader_val, device)
                
                mae, mse = result[0], result[1]
                mae_list.append(mae)
                best_mae = np.min(mae_list)
                
                eval_log = f"--- [Eval] Epoch {epoch} | MAE: {mae:.2f} | MSE: {mse:.2f} | Best MAE: {best_mae:.2f}"
                print(eval_log)
                with open(run_log_name, "a") as f: f.write(eval_log + "\n")
                
                writer.add_scalar('metric/mae', mae, epoch)
                writer.add_scalar('metric/mse', mse, epoch)

                if mae <= best_mae:
                    torch.save({'model': model_without_ddp.state_dict(), 'epoch': epoch, 'mae': mae}, 
                               os.path.join(args.checkpoints_dir, 'best_mae.pth'))
                    print(f"ðŸ”¥ ìµœê³  ì„±ëŠ¥ ê°±ì‹  ì™„ë£Œ.")

        except RuntimeError as e:
            if 'out of memory' in str(e):
                print(f"âš ï¸ OOM ë°œìƒ. ì—í­ {epoch}ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                gc.collect()
                torch.cuda.empty_cache()
                continue
            else: 
                raise e

    writer.close()

if __name__ == '__main__':
    args = get_args_parser().parse_args()
    main(args)