import argparse
import os
import torch
from torch.utils.data import DataLoader
import util.misc as utils
from models import build_model
from crowd_datasets import build_dataset
from engine import evaluate_crowd_no_overlap
import warnings

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')

def get_args_parser():
    parser = argparse.ArgumentParser('Set parameters for testing P2PNet', add_help=False)
    
    # ê¸°ë³¸ ì„¤ì • (í•™ìŠµ ì‹œì™€ ë™ì¼í•˜ê²Œ ë§ì¶°ì•¼ í•¨)
    parser.add_argument('--backbone', default='vgg16_bn', type=str, help="Name of the convolutional backbone to use")
    parser.add_argument('--row', default=2, type=int, help="row number of anchor points")
    parser.add_argument('--line', default=2, type=int, help="line number of anchor points")
    
    # ë°ì´í„°ì…‹ ë° ê²½ë¡œ ì„¤ì •
    parser.add_argument('--dataset_file', default='SHHA', help='dataset name (SHHA or SHHB)')
    parser.add_argument('--data_root', default='/home/mingi/Downloads/SHT', help='path where the dataset is')
    
    # ëª¨ë¸ ê°€ì¤‘ì¹˜ ê²½ë¡œ (í•„ìˆ˜ ì…ë ¥)
    parser.add_argument('--weight_path', default='./weights/best_mae.pth', type=str, help='path to the trained model checkpoint')
    
    # ê¸°íƒ€ ì„¤ì •
    parser.add_argument('--gpu_id', default=0, type=int, help='the gpu used for testing')
    parser.add_argument('--num_workers', default=8, type=int)
    
    # ëª¨ë¸ ë¹Œë“œì— í•„ìš”í•œ ë”ë¯¸ ì¸ìë“¤ (í…ŒìŠ¤íŠ¸ì—” ì•ˆ ì“°ì´ì§€ë§Œ build_model í˜¸ì¶œ ì‹œ í•„ìš”)
    parser.add_argument('--lr', default=1e-4, type=float)
    parser.add_argument('--lr_backbone', default=1e-5, type=float)
    parser.add_argument('--batch_size', default=1, type=int)
    parser.add_argument('--weight_decay', default=1e-4, type=float)
    parser.add_argument('--epochs', default=3500, type=int)
    parser.add_argument('--lr_drop', default=3500, type=int)
    parser.add_argument('--clip_max_norm', default=0.1, type=float)
    parser.add_argument('--frozen_weights', type=str, default=None)
    parser.add_argument('--set_cost_class', default=1, type=float)
    parser.add_argument('--set_cost_point', default=0.05, type=float)
    parser.add_argument('--point_loss_coef', default=0.0002, type=float)
    parser.add_argument('--eos_coef', default=0.5, type=float)

    return parser

def main(args):
    # GPU ì„¤ì •
    os.environ["CUDA_VISIBLE_DEVICES"] = '{}'.format(args.gpu_id)
    device = torch.device('cuda')

    # ëª¨ë¸ ë¹Œë“œ
    print(f"Loading model from {args.weight_path}...")
    
    # [ìˆ˜ì •] build_model ë°˜í™˜ê°’ ì²˜ë¦¬ (ë‹¨ì¼ ê°ì²´ ë°˜í™˜ ì‹œ ì–¸íŒ¨í‚¹ ì˜¤ë¥˜ ë°©ì§€)
    res = build_model(args, training=False)
    if isinstance(res, tuple):
        model, _ = res
    else:
        model = res

    model.to(device)

    # ê°€ì¤‘ì¹˜ ë¡œë“œ
    if os.path.exists(args.weight_path):
        checkpoint = torch.load(args.weight_path, map_location='cpu')
        
        # DataParallelë¡œ ì €ì¥ëœ ê²½ìš° 'module.' ì ‘ë‘ì‚¬ ì œê±°
        if 'model' in checkpoint:
            state_dict = checkpoint['model']
        else:
            state_dict = checkpoint # state_dictë§Œ ì €ì¥ëœ ê²½ìš° ëŒ€ë¹„
            
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith('module.'):
                new_state_dict[k[7:]] = v
            else:
                new_state_dict[k] = v
                
        model.load_state_dict(new_state_dict)
        print("âœ… Model weights loaded successfully.")
    else:
        print(f"âŒ Error: Checkpoint not found at {args.weight_path}")
        return

    # ë°ì´í„°ì…‹ ë¡œë“œ (Validation Set = Test Set)
    loading_data = build_dataset(args=args)
    _, val_set = loading_data(args.data_root) # val_setì´ ê³§ Test Setì„
    
    # ë°ì´í„° ë¡œë” (TestëŠ” batch_size=1 ê¶Œì¥)
    sampler_val = torch.utils.data.SequentialSampler(val_set)
    data_loader_val = DataLoader(val_set, 1, sampler=sampler_val,
                                    drop_last=False, collate_fn=utils.collate_fn_crowd, num_workers=args.num_workers)

    print(f"Start Testing on {len(val_set)} images...")
    
    # í‰ê°€ ìˆ˜í–‰
    model.eval() # í‰ê°€ ëª¨ë“œ í™•ì‹¤íˆ ì„¤ì •
    mae, mse = evaluate_crowd_no_overlap(model, data_loader_val, device)
    
    print("\n" + "="*40)
    print(f"ğŸ† Final Test Result for {args.dataset_file}")
    print(f"   MAE: {mae:.2f}")
    print(f"   MSE: {mse:.2f}")
    print("="*40 + "\n")

if __name__ == '__main__':
    parser = argparse.ArgumentParser('P2PNet testing script', parents=[get_args_parser()])
    args = parser.parse_args()
    main(args)
